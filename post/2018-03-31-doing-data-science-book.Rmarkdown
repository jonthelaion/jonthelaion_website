---
title: Doing Data Science (Book)
author: jonthelaion
date: '2018-04-01 17:30:00'
slug: doing-data-science-book
categories:
  - R
  - datascience
tags:
  - R
  - datascience
---

With years of experience in both academia and industry, authors Cathy O'Neil & Rachel Schutt look beyond the hype to explore what the data science discipline entails and looks like in practice. 

They also explore an idea that is often overlooked when speaking about data science - that building models and working with data is not value-neutral. You choose the problems you will work on, you make assumptions in those models, you choose metrics, and you design the algorithms.

<a href="https://github.com/oreillymedia/doing_data_science" target="_blank">Supplementary Material</a>



# Chapter 1 - Introduction: What Is Data Science?

While there is not a clear definition of what a data scientist is, they generally have knowledge and experience in the following key areas:

- Computer science
- Math
- Statistics
- Machine learning
- Domain expertise
- Communication and presentation skills
- Data visualisation

In practice, data science problems are formed by teams with individuals from various backgrounds (e.g. statistician, software engineers, social scientists) and with various degrees of capabilities in the 7 areas identified above. Depending on the industry and problem at hand, certain skills may be more or less important.

An academic data scientist works with large amounts of data, and must grapple with computational problems posed by the structure, size, messiness, and the complexity and nature of the data, while simultaneously solving a real world problem. They can be trained in anything from social science to biology.

An industry data scientist is someone who knows how to extract meaning from and interpret data, which requires both tools and methods from statistics and machine learning, as well as being human. Collecting, cleaning and munging data is usually the first step as data is rarely ever clean in the real world. This requires persistence, statistics and software engineering skills - which are also necessary for understanding biases in the data, and for debugging logging output from code.

Once the data is brought into shape, exploratory data analysis is performed, which combines visualisation and data sense. Patterns are identified, models are built and aglorithms written to both understand the data as well as prototyping for deployment purposes. Experiments may need to be designed to drive decision making and throughout of all this there will be continual communication with team members, engineers and leadership.

# Chapter 2 - Statistical Inference, Exploratory Data Analysis, and the Data Science Process

## Statistical Inference

The world we live in is complex, random and uncertain. Data, which represents the traces of real-world processes, is continually being generated and collected and exactly which traces we gather are decided by our data collection or sampling methods. It is important to remember that collected data is not always complete nor entirely objective due to the randomness and uncertainty underlying the process itself, and the uncertainty associated with underlying data collection methods.

**Statistical inference** is the discipline that concerns itself with the development of procedures, methods and theorems that allow us to extract meaning and informaiton from data that has been generated by stochastic (random) processes.

**Population** (represented by N) is the complete set of observations to answer the question at hand.

**Sample** (represented by n) is a subset of N that is taken in order to draw concusions and make inferences about the population. There are different ways to sample and it is important to be always aware of the sampling mechanism used because it can introduce biases into the data, which can distort any conclusions drawn.

**Sampling distribution** is the inherent uncertainty from random sampling. Even if we use the same sampling methodology, we will not get the exact same emails the second time around.

A common misconception of "Big Data" is that we do not need to be concerned with sampling methodologies now that we are capable of handling such large amounts of data (i.e. N = n = All). However, in reality, it is pretty much never all and very often missing the very things that we should care about most (e.g. people who do not show up election night polls to understand voting problems).

Models that ignore causation can add to historical problems instead of addressing them (e.g. model to select job candidates based on past selections can reinforce gender biases). Data does not speak for itself.

A **model** is an artificial construction where all extraneous detail has been removed or abstracted in order to understand and represent the nature of reality through a particular lens. Attention must always be paid to those abstracted details after a model has been analysed to see what might have been overlooked.

## Exploratory Data Analysis (EDA)

Once you have data, one of the first things to do is Exploratory Data Analysis (EDA).

## Data Science Process