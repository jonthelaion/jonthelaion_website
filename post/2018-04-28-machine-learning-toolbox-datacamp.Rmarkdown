---
title: Machine Learning Toolbox (DataCamp)
author: jonthelaion
date: '2018-04-28'
slug: machine-learning-toolbox-datacamp
categories:
  - datascience
  - machinelearning
  - R
  - datacamp
tags:
  - datascience
  - machinelearning
  - R
  - datacamp
---

# Supervised Learning

- The `caret` package in R automates supervised learning (a.k.a. predictive modelling) where the target variable is available.
- There are two types of predictive models:
 - Classification: Qualitative target variable
 - Regression: Quantitative target variable
- An important step for both classification and regression models is to use quantifiable and objectic metrics to evaluate the accuracy of the model.

## Regression

- Root Mean Squared Error (RMSE) is a common metric used for regression (e.g. `lm()`)
- Common to evaluate in-sample RMSE, but this leads to overfitting
- Better to calculate out-of-sample error (`caret`), which simulates real-world usage and helps avoid overfitting.

#### Example: In-Sample Error

```{r}
data(mtcars)
model <- lm(mpg ~ hp, mtcars[1:20, ])

predicted <- predict(model, mtcars[1:20, ], type='response')

RMSE <- sqrt(mean((predicted - mtcars$mpg[1:20])^2))
RMSE
```

#### Example: Out-of-Sample Error

```{r}
data(mtcars)
model <- lm(mpg ~ hp, mtcars[1:20, ])

predicted <- predict(model, mtcars[21:32, ], type='response')

RMSE <- sqrt(mean((predicted - mtcars$mpg[21:32])^2))
RMSE
```

- We can see that the linear model has not generalised as well to data that was not used in training the original model. 
- Sometimes data is sorted according to a particular variable (e.g. `mpg` from lowest to highest) so it is better to shuffle the order of the rows first before splitting.

#### Example: Single Train/Test Split Error

```{r}
# Shuffle the data
set.seed(42)
rows <- sample(nrow(mtcars))
mtcars2 <- mtcars[rows, ]
splitrow <- round(nrow(mtcars2) * 0.80)
train <- mtcars2[1:splitrow, ]
test <- mtcars2[(splitrow+1):nrow(mtcars2), ]

model <- lm(mpg ~ ., train)
predicted <- predict(model, test)
RMSE <- sqrt(mean((predicted - test$mpg)^2))
RMSE
```

### Cross Validation

- Cross validation invovles re-arranging the full data set into multiple configuration (or folds), with each point in the original data set appearing once within each fold.
- Training is performed across all folds and then the model is fit onto the full dataset.
- THis makes CV very expensive - it is effectively 11x as expensive as fitting a single model for 10-fold CV.
- You can also do multiple iterations of CV (e.g. 5 x 5-fold CV) by using the `repeats` argument.
- `caret` package uses bootstrapping as an alternate method to CV, with similar results.

```{r}
library(caret)
data(mtcars)
set.seed(42)

model <- train(mpg ~ ., mtcars,
               method = "lm",
               trControl = trainControl(
                   method = "cv", number = 5, repeats = 5,
                   verboseIter = TRUE
               )
)
model

predicted <- predict(model, mtcars)

RMSE <- sqrt(mean((predicted - mtcars$mpg)^2))
RMSE
```

## Classification

- Categorical (i.e. qualitative) target variable
- Example: will a loan default?
- Still a form of supervised learning
- Use a train/test split to evaluate performance
- A common dataset for classification problems is the `Sonar` dataset from the `mlbench` package which can be used to train a model to distinguish rocks \(R\) from mines (M).

### Confusion Matrix

- A very useful tool for evaluating the effectiveness of models with binary outcomes.
- The columns (Reference) represent the true outcomes while the rows (Prediction) represent the predicted outcomes.
- The main diagonal are the cases where the model is correct (true positive and true negative) and the other diagonal are the cases where the model is incorrect (false positive and false negative).
- The threshold can be tweaks to adjust the probabilities of true positive (higher chance of predicting it is a mine) and true negatives (higher chance of correctly predicting it is a rock). 
- This is a cost-benefit question and a ROC curve is a tool that is often used to find the desired threshold.
- A ROC curve plots true/false positive at every possible threshold and allows for visualisation of tradeoff between the two extremes.
- The Area under the Curve (AUC) is a single number summary of model accuracy. It summarises performance across all thresholds and can be used to rank different models within the same dataset.
- AUC range from 0 to 1:
 - 0.5 = random guessing
 - 1 = model is always right
 - 0 = model is always wrong
- Rule of thumb is that AUC can be thought of as a letter grade:
 - 0.9 = "A"
 - 0.8 = "B"

<img src="/img/confusion_matrix.jpg" alt="">

#### Example: Building glm() classification model and confusion matrix

```{r}
library(mlbench)
library(caTools)
data(Sonar)

# Have a look at the Sonar data
Sonar[1:10, c(1:5, 61)]

# Shuffle the rows
mixed_rows <- sample(nrow(Sonar))
Sonar2 <- Sonar[mixed_rows, ]

# Have a look at the shuffled Sonar data
Sonar2[1:10, c(1:5, 61)]

# Splitting into train/test sets in 60/40
split_row <- round(nrow(Sonar2) * 0.60)
train <- Sonar2[1:split_row, ]
test <- Sonar2[(split_row + 1):nrow(Sonar2), ]

# Training model and generating predictions
model <- glm(Class ~ ., family=binomial(link = "logit"), train)
predicted <- predict(model, test, type="response")

# View summary
summary(predicted)

# Turn probabilities into classes and look at their frequencies
p_class <- ifelse(predicted > 0.50, "M", "R")
table(p_class)

# Use caret's helper function to calculate additional statistics
confusionMatrix(p_class, test[["Class"]])

# colAUC() function to generate ROC curve
colAUC(predicted, test$Class, plotROC = TRUE)

# Create trainControl object: myControl
myControl <- trainControl(
  method = "cv",
  number = 10,
  summaryFunction = twoClassSummary,
  classProbs = TRUE, # IMPORTANT!
  verboseIter = TRUE
)

# Train glm with custom trainControl: model
model <- train(Class ~ ., Sonar, method = "glm", trControl = myControl)

# Print model to console
model
```

## Random Forests

- Random forests are a popular type of machine learning model that is good for beginners. They are robust to overfitting and can yield very accurate, non-linear models.
- Unlike linear models, they have hyperparameters which require manual specification. This can impact model fit and vary from dataset to dataset. Default values are often OK, but occassionally they will require adjustment.
- Random forests start with simple decision trees and then improve accuracy by fitting many trees. Each one is fit to a bootstrap sample of data (a.k.a. bootstrap aggregation or bagging).
- Randomly sample columns at each split.

```{r}
library(ranger)
set.seed(42)

# Fit random forest: model
model <- train(Class~., Sonar, method = "ranger", trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE))

# Print model to console
model

# Plot model
plot(model)
```

