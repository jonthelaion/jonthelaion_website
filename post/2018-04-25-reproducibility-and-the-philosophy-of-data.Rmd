---
title: Reproducibility and the Philosophy of Data
author: jonthelaion
date: '2018-04-25 20:26:00'
slug: reproducibility-and-the-philosophy-of-data
categories:
  - podcast
  - twimlai
tags:
  - podcast
  - twimlai
---

Guest: Clare Gollnick

Link: <a href="https://twimlai.com/twiml-talk-121-reproducibility-philosophy-data-clare-gollnick/" target="_blank">TWIMLAI Talk #121</a>

A very interesting talk about the reproducibility controversy that is rocking the scientific community. A "Nature" survey in 2016 showed that more than 70% of researchers have tried and failed to reproduce another scientist's experiments, and more than half have failed to reproduce their own experiments.

To a certain extent, this could be a result of the reliance on the p-value as the test of statistical significance in scientific studies. This reliance exposes studies to the risk of "p-hacking", where scientists re-run and re-analyse data until a sufficiently low p-value is obtained. 

Interestingly, this iterative process parallels the way that data science is often used to generate predictive models. Overfitting on the test set in data science is the equivalent to p-hacking in scientific studies. However, the cross-validation step of testing the predictive model against a validation data set that is completely separate to the training data is a key step to help mitigate this risk.

There is a field of study known as the philosphy of data which explores why it is that we believe we can learn from data. Because 100% of the data we collect is data about the past, one implicit assumption that we make is that there is a set of shared rules about the past and the future i.e. that due to these shared rules, data about the past can be generalised and used to predict the future. If data from the past was due to random chance, then it would not be of any use for understanding or predicting the future.

In practice, it is important to think about this core assumption when "framing the problem".