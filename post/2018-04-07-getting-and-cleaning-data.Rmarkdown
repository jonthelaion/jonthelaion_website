---
title: Getting and Cleaning Data
author: jonthelaion
date: '2018-04-07 20:00:00'
slug: getting-and-cleaning-data
categories:
  - R
  - datascience
  - coursera
  - mooc
tags:
  - R
  - coursera
  - datascience
  - mooc
---

# Raw versus Processed Data

- Raw data
 - The original source of the data
 - Often hard to use for data analysis
 - Data analysis includes processing
 - Raw data may only need to be processed once
- Processed data
 - Data that is ready for analysis
 - Processing can include merging, subsetting, transforming, etc.
 - There may be standards for processing
 - **All steps should be recorded.**
 
# The components of tidy data

- The four things you should have:
 1. The raw data
 2. The tidy data set
 3. A code book describing each variable and its values in the tidy data set (a.k.a. metadata)
 4. An explicit and exact recipe you used to go from 1 -> 2,3.
- **Raw data**
 - Ran no software on the data
 - Did not manipulate any of the numbers in the data
 - You did not remove any data from the data set
 - You did not summarise the data in any way
- **Tidy data**
 - Each variable you measure should be in one column
 - Eech different observation of that variable should be in a different row
 - There should be one table for each "kind" of variable
 - If you have multiple tables, they should include a column in the table that allows them to be linked
- Some other important tips when processing data:
 - Include a row at the top of each file with variable names
 - Make variable names human readable e.g. AgeAtDiagnosis instead of AgeDx
 - In general data should be saved in one file per table
- **The code book**
 - Information about the variables (including units) in the data set not contained in the tidy data
 - Information about the summary choices you made
 - Information about the experimental study design you used
- Some other important tips:
 - A common format for this document is Word/txt file (or Markdown file)
 - There should be a section called "Study Design" that has a thorough description of how you collected the data
 - There must be a section called "Code Book" that describes each variable and its units
- **Instruction List**
 - Ideally a computer script
 - The input for the script is the raw data
 - The output is the processed, tidy data
 - There are no parameters to the scriptx
 - In some cases, it will not be possible to script every step. In those situations, it is important to be as explicit and detailed as possible, including version numbers, parameter values etc.

# Getting Data

- There are a few principal functions for reading data into R:
 - `read.table()`, `read.csv()`: for reading tabular data
 - `readLines()`: for reading lines of a text file
 - `source()`: for reading in R code files (inverse of `dump()`)
 - `dget()`: for reading R code files (inverse of `dput()`)
 - `load()`: for reading in saved workspaces
 - `unserialize()`: for reading single R objects in binary form
- In general, when using R with larger datasets, it's useful to know a few things about your system:
 - How much memory is available?
 - What other applications are in use?
 - Are there any other users logged into the same system?
 - What operating system?
 - Is it OS 32 or 64 bit?
- Calculating memory requirements:
 - Data frame with 1.5 million rows and 120 columns, all of which are numeric data. Roughly, how much memory is required to store this data frame? 1,500,000 x 120 x 8 bytes/numeric = 11,440,000,000bytes / 220bytes/MB = 1,373.29MB = 1.34GB
 - Rule of thumb is that you will need rougly twice as much RAM.



## Downloading Files

- A basic component of working with data is getting/setting your working directory.
- `getwd()` and `setwd()` are two functions that do this.
- Be aware of relative vs absolute path:
 - Relative: `setwd("./data")`, `setwd("../")`
 - Absolute: `setwd("/Users/Jon/data")`, `setwd("C:\\Users\\Jon\\Downloads")`
 
### Example: Checking to see if directory exists and creating it if it doesn't

```{r}
# if (!file.exists("data")) {
#    dir.create("data")
# }
```

- While you can right-click and save to download a file, using a script (e.g. the `download.file()` function) will aid in reproducibility.
- Important parameters are `url`, `destfile` and `method`.

### Example: Downloading files using script

```{r prompt=TRUE}
# fileUrl <- "https://data.gov.au/dataset/f2b7c2c1-f4ef-4ae9-aba5-45c19e4d3038/resource/2156cb99-3358-4847-8b5b-fcd2f0d3c4e2/download/financialadvisers201804.xlsx"
# download.file(fileUrl, destfile="./data/financialadvisers201804.xlsx", method="curl")
# list.files("./data")

# dateDownloaded <- date()
# dateDownloaded
```

## Reading local flat files

- `read.table()` is the main function for reading data into R.
- It is flexible and robust but requires more parameters.
- Important parameters:
 - `file`: the name of a file or a connection
 - `header`: logical indicating if the file has a header line
 - `sep`: a string indicating how the columns are separated
 - `row.names`
 - `colClasses`: a character vector indicating the class of each column in the dataset
 - `nrows`: the number of rows in the dataset
 - `comment.char`: a character string indicating the comment character
 - `skip`: the number of lines to skip from the beginning
 - `quote`: one of the biggest troubles with reading flat files are quotation marks ' or " placed in data values, setting `quote=""` often resolves this
 - `na.strings`
 - `stringAsFactors`: should character variables be coded as factors? Default is 'TRUE'.
- Related: `read.csv()`, `read.csv2()`
- It reads data into RAM so big datasets can cause problems.
- For larger datasets, doing the following things will make your life easier and will prevent R from choking:
 - Read the help page for `read.table()`, which contains many hints.
 - Make a rough calculation of the memory required to store your dataset. If the dataset is larger than the amount of RAM on your computer, you can probably stop right here.
 - Set `comment.char = ""` if there are no commented lines in your file.
 - Set `nrows`. Thi sdoesn't make R run faster, but it helps with memory usage. A mild overestimate is OK. You can use the Unix tool `wc` to calculate the number of lines in a file.
 - Use the `colClasses` argument. Specifying this option instead of using the default can make `read.table()` run MUCH faster, often twice as fast. In order to use this option, you have to know the class of each column in your data frame. If all of the columns are "numeric", for example, then you can just set `colClasses = "numeric"`. A quick and dirty way to figure out the classes of each column is the following:
 
```{r}
# initial <- read.table("datatable.txt", nrows = 100)
# classes <- sapply(initial, class)
# tabAll <- read.table("datatable.txt", colClasses = classes)
```

 
## Reading Excel files
 
- Still most widely used format for sharing data
- In the `xlsx` package, use the `read.xlsx()`, `read.xlsx2()` functions.
- Parameters used include: `sheetIndex`, `header`
- Can read specific rows and columns by setting up variables e.g. `colIndex <- 2:3` and `rowIndex <- 1:4` then `read.xlsx("data.xlsx", sheetIndex=1, colIndex=colIndex, rowIndex=rowIndex)`
- `write.xlsx()` function will write out an Excel file with similar arguments.
- `read.xlsx2()` is much faster than `read.xlsx()` but for reading subsets of rows may be slightly unstable
- The `XLConnect` package has more options for writing and manipulating Excel files.
- The **XLConnect vignette** is a good place to start for that package.
- In general, it is advised to store your data in either a database or in comma separated files (.csv) or tab separated (.tab/.txt) as they are easier to distribute.
 
## Reading XML

- f
 
# Writing Data

- There are analogous functions for writing data to files:
 - `write.table()`
 - `writeLines()`
 - `dump()`
 - `dput()`
 - `save()`
 - `serialize()`
- `dump()` and `dput()` are useful because the resulting textual format is edit-able, and in the case of corruption, potentially recoverable
- Unlike writin gout a table or csv file, these functions preserve the metadata (sacrificing some readibility) so that the user doesn't have to specify it all over again
- Textual formats can work much better with version control programs like subversion or git which can only track changes meaningfully in text files.
- Textual formats adhere to the "UNIX philosophy".
- The downside is that the format is not very space efficient.

**Example:** `dput()`

```{r}
# y <- data.frame(a=1, b="a")
# dput(y)
# dput(y, file="y.R")
# new.y <- dget("y.R")
# new.y
```

